{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./processed_metrics.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['switch', 'timestamp', 'Average Flow Duration ',\n",
       "       'Average Packets Per Flow ', 'Bridge Controller Status',\n",
       "       'CPU Utilization', 'Rate of Packet in Messages ',\n",
       "       'Rate of Port Flapping ', 'Interface Utilization', 'label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"label\", \"timestamp\", \"switch\", \"Interface Utilization\"])\n",
    "Y = df[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in X.iterrows():\n",
    "    min_len = 99999\n",
    "    for col in X.columns:\n",
    "        a = row[col][1:-1].split(\",\")\n",
    "        if \"\" in a:\n",
    "            data_points_len = a.index(\"\")\n",
    "            min_len = min(data_points_len, min_len)\n",
    "        else:\n",
    "            min_len = min(data_points_len, len(a))\n",
    "    \n",
    "    for col in X.columns:\n",
    "        try:\n",
    "            row[col] = [float(x) for x in row[col][1:-1].split(\",\")[:min_len]]\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(col, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 5.666666666666667,\n",
       " 11.0,\n",
       " 20.6,\n",
       " 25.266666666666666,\n",
       " 20.583333333333332,\n",
       " 15.875,\n",
       " 18.75,\n",
       " 11.909090909090908,\n",
       " 18.153846153846153,\n",
       " 18.923076923076923,\n",
       " 18.923076923076923,\n",
       " 19.818181818181817,\n",
       " 27.181818181818183,\n",
       " 26.454545454545453,\n",
       " 15.375,\n",
       " 18.125,\n",
       " 12.363636363636363,\n",
       " 14.181818181818182,\n",
       " 4.2,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.iloc[0][\"Average Packets Per Flow \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95, 24, 6)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_series_data = []\n",
    "\n",
    "for _, row in X.iterrows():\n",
    "    stacked = np.stack([row[col] for col in X.columns])\n",
    "\n",
    "    time_series_data.append(stacked.T)\n",
    "\n",
    "len(time_series_data), len(time_series_data[10]), len(time_series_data[10][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "label_map = {0: 0, 2: 1, 3: 2}\n",
    "y = np.array([label_map[label] for label in df[\"label\"].astype(int).values])\n",
    "\n",
    "y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    time_series_data,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# batch_X_list = [torch.tensor(x, dtype=torch.float32) for x in time_series_data]\n",
    "\n",
    "# lengths = [len(x) for x in batch_X_list]\n",
    "\n",
    "# X_padded = torch.nn.utils.rnn.pad_sequence(batch_X_list, batch_first=True)\n",
    "# packed = torch.nn.utils.rnn.pack_padded_sequence(X_padded, lengths, batch_first=True, enforce_sorted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = [torch.tensor(seq, dtype=torch.float32) for seq in sequences]\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.labels[idx]\n",
    "\n",
    "# Collate function for padding\n",
    "def collate_fn(batch):\n",
    "    batch.sort(key=lambda x: len(x[0]), reverse=True)  # sort by sequence length\n",
    "    sequences, labels = zip(*batch)\n",
    "\n",
    "    lengths = [len(seq) for seq in sequences]\n",
    "    padded_seqs = pad_sequence(sequences, batch_first=True)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    return padded_seqs, lengths, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TimeSeriesDataset(X_train, y_train)\n",
    "test_dataset = TimeSeriesDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes, num_layers=2, dropout=0.3, bidirectional=True):\n",
    "        super().__init__()\n",
    "        self.bidirectional = bidirectional\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size * (2 if bidirectional else 1), num_classes)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        # Pack padded sequence\n",
    "        packed = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=True)\n",
    "        packed_output, (hn, cn) = self.lstm(packed)\n",
    "\n",
    "        # hn shape: (num_layers * num_directions, batch, hidden_size)\n",
    "        # Get the last layer's hidden states for both directions if bidirectional\n",
    "        if self.bidirectional:\n",
    "            # Concatenate the final forward and backward hidden states\n",
    "            last_hidden = torch.cat((hn[-2], hn[-1]), dim=1)\n",
    "        else:\n",
    "            last_hidden = hn[-1]\n",
    "\n",
    "        out = self.dropout(last_hidden)\n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = X_train[0].shape[1]      # Number of features\n",
    "hidden_size = 64\n",
    "num_classes = len(set(y_train.numpy()))   # Or however many classes you have\n",
    "\n",
    "model = RNNClassifier(input_size, hidden_size, num_classes+1)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 4.2227\n",
      "Epoch 2, Loss: 3.8716\n",
      "Epoch 3, Loss: 3.4819\n",
      "Epoch 4, Loss: 3.0215\n",
      "Epoch 5, Loss: 2.6432\n",
      "Epoch 6, Loss: 2.2366\n",
      "Epoch 7, Loss: 1.6954\n",
      "Epoch 8, Loss: 1.4837\n",
      "Epoch 9, Loss: 1.1050\n",
      "Epoch 10, Loss: 1.2251\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_x, lengths, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch_x, lengths)\n",
    "        loss = criterion(output, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 89.47%\n",
      "F1 Score: 0.8457\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "model.eval()\n",
    "correct, total = 0, 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_x, lengths, batch_y in test_loader:\n",
    "        output = model(batch_x, lengths)\n",
    "        preds = output.argmax(dim=1)\n",
    "\n",
    "        correct += (preds == batch_y).sum().item()\n",
    "        total += batch_y.size(0)\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(batch_y.cpu().numpy())\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "f1 = f1_score(all_labels, all_preds, average='macro')  # use 'weighted' if you have imbalanced classes\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
